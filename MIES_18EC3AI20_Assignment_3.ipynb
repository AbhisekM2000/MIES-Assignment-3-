{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9c624d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Dense_layer():\n",
    "    def __init__(self, input_features, layer1_nodes, output_labels, learning_rate, iterations):\n",
    "        self.weights_layer1 = np.random.rand(input_features,layer1_nodes)*0.1      #8x8 dimension\n",
    "        self.bias_layer1 = np.zeros((1,layer1_nodes)) #1x8 dimension\n",
    "        self.weights_layer2 = np.random.rand(layer1_nodes,output_labels)*0.1  #8x3 dimension\n",
    "        self.bias_layer2 = np.zeros((1,output_labels)) #1x3 dimension\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def sigmoid(self, input):\n",
    "        return 1/(1+np.exp(-input))\n",
    "    \n",
    "    def sigmoid_derivative(self,input):\n",
    "        return np.exp(-input) / ((1 + np.exp(-input)) ** 2)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        # The below is the output of layer 1 (hidden layer ), first its passed through a linear layer and then sigmoid activated\n",
    "        self.output_layer1 = np.dot(input,self.weights_layer1)+self.bias_layer1\n",
    "        self.activated_output_layer1 = self.sigmoid(self.output_layer1)\n",
    "        \n",
    "        # The below is the output of layer 2, first its passed through a linear layer and then sigmoid activated\n",
    "        self.output_layer2 = np.dot(self.activated_output_layer1,self.weights_layer2)+self.bias_layer2\n",
    "        self.activated_output_layer2 = self.sigmoid(self.output_layer2)\n",
    "        \n",
    "        return np.argmax(self.activated_output_layer2)\n",
    "\n",
    "    def calc_loss(self, predicted_label, actual_label):\n",
    "        return np.sum(np.square((predicted_label-actual_label)))\n",
    "\n",
    "    def backward(self, input, y_pred, y_actual):\n",
    "\n",
    "        # Calculation of the derivatives of loss\n",
    "        self.loss_derivative2 = np.multiply((y_pred-y_actual), self.sigmoid_derivative(self.output_layer2)) \n",
    "        self.loss_derivative1 = np.dot(self.loss_derivative2, self.weights_layer2.T)*self.sigmoid_derivative(self.output_layer1)\n",
    "        \n",
    "        self.layer2_weight_derivative = np.dot(self.activated_output_layer1.T,self.loss_derivative2)\n",
    "        self.layer2_bias_derivative = 1/input.shape[0]*(np.sum(self.loss_derivative2,axis=0).reshape(1,3))\n",
    "        \n",
    "        self.layer1_weight_derivative = np.dot(input.T,self.loss_derivative1)\n",
    "        self.layer1_bias_derivative = 1/input.shape[0]*(np.sum(self.loss_derivative1,axis=0).reshape(1,8))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def update_parameters(self,dW1,db1,dW2,db2):\n",
    "        self.weights_layer1 -= (self.learning_rate *dW1)\n",
    "#         print(self.bias_layer1.shape)\n",
    "#         print(db1.shape)\n",
    "        self.bias_layer1 -= (self.learning_rate*db1)\n",
    "        self.weights_layer2 -= (self.learning_rate *dW2)\n",
    "        self.bias_layer2 -= (self.learning_rate*db2)\n",
    "        # print(\"Weights are updated\")\n",
    "\n",
    "    def train(self, input, y_actual):\n",
    "        self.forward(input)\n",
    "        loss = self.calc_loss(self.activated_output_layer2, y_actual)\n",
    "        self.backward(input, self.activated_output_layer2, y_actual)\n",
    "        self.update_parameters(self.layer1_weight_derivative,self.layer1_bias_derivative,self.layer2_weight_derivative,self.layer2_bias_derivative)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def check_output(self, input, y_actual):\n",
    "        pred_output = self.forward(input)\n",
    "        actual_output = np.argmax(y_actual)\n",
    "        if actual_output == pred_output:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f6153",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fe3b983a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after 50th iteration is = 0.332325944662244\n",
      "The loss after 100th iteration is = 0.3321565760165335\n",
      "The loss after 150th iteration is = 0.3315333220602426\n",
      "The loss after 200th iteration is = 0.3293544277427394\n",
      "The loss after 250th iteration is = 0.3233667652791529\n",
      "The loss after 300th iteration is = 0.31298151965419985\n",
      "The loss after 350th iteration is = 0.3015709916534162\n",
      "The loss after 400th iteration is = 0.29206312825685343\n",
      "The loss after 450th iteration is = 0.28515329841953246\n",
      "The loss after 500th iteration is = 0.28039257661906736\n",
      "0.5450191570881227\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Snails.csv')\n",
    "\n",
    "def splitData(data, split_ratio = [0.50, 0.50]):\n",
    "\n",
    "    '''\n",
    "        The ratio for the split is kept as 80:20 that is 80 pecent of the data will \n",
    "        be divided into the training test and 20 percent of the data in the testing set.\n",
    "        Here validation set is necessary for pruning the decison tree constructed\n",
    "    ''' \n",
    "    train =  data.sample(frac=split_ratio[0])\n",
    "    val = data.drop(train.index)\n",
    "    return (train, val)\n",
    "\n",
    "train, test = splitData(df)\n",
    "test.reset_index(inplace = True, drop = True)\n",
    "train.reset_index(inplace = True, drop = True)\n",
    "res = train['sex']\n",
    "\n",
    "train = train.drop('sex', axis = 1)\n",
    "train = train.drop('Id', axis = 1)\n",
    "\n",
    "col = train.columns\n",
    "train = train.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "train = (train  - train.min(axis = 0))/ (train.max(axis=0) - train.min(axis = 0))\n",
    "fin = np.zeros((len(train), 3))\n",
    "for i in range(len(train)):\n",
    "    if res[i] == 'M':\n",
    "        fin[i][0] = 1\n",
    "    elif res[i] == 'F':\n",
    "        fin[i][1] = 1\n",
    "    else:\n",
    "        fin[i][2] = 1\n",
    "        \n",
    "        \n",
    "\n",
    "neural_net = Dense_layer(input_features=8, layer1_nodes=8,output_labels=3, learning_rate=0.001, iterations=500)\n",
    "\n",
    "for j in range(500):\n",
    "    \n",
    "    loss=neural_net.train(train,fin)\n",
    "    loss=(0.5*loss)/train.shape[0]\n",
    "    if (j+1) % 50 == 0:\n",
    "        print(f'The loss after {j+1}th iteration is = {loss}')\n",
    "\n",
    "\n",
    "neural_net.forward(train)\n",
    "pred=neural_net.activated_output_layer2\n",
    "hit = 0\n",
    "for i in range(len(train)):\n",
    "    if np.where(pred[i] == max(pred[i])) == np.where(fin[i] == max(fin[i])):\n",
    "        hit += 1\n",
    "print(hit/len(train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ca036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
